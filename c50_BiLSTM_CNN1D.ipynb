{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "With 80-20 train and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 files belonging to 50 classes.\n",
      "Found 2500 files belonging to 50 classes.\n",
      "Using 500 files for validation.\n",
      "Found 2500 files belonging to 50 classes.\n",
      "Using 2000 files for training.\n"
     ]
    }
   ],
   "source": [
    "import keras\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from pathlib import Path\r\n",
    "from utils import plot_history\r\n",
    "from keras import models, layers\r\n",
    "from keras.preprocessing import text_dataset_from_directory\r\n",
    "from keras.layers.experimental.preprocessing import TextVectorization\r\n",
    "\r\n",
    "ds_dir = Path('data/C50/')\r\n",
    "train_dir = ds_dir / 'train'\r\n",
    "test_dir = ds_dir / 'test'\r\n",
    "seed = 123\r\n",
    "batch_size = 32\r\n",
    "\r\n",
    "train_ds = text_dataset_from_directory(\r\n",
    "    train_dir,\r\n",
    "    label_mode='categorical',\r\n",
    "    seed=seed,\r\n",
    "    shuffle=True,\r\n",
    "    batch_size=batch_size)\r\n",
    "\r\n",
    "val_ds = text_dataset_from_directory(\r\n",
    "    test_dir,\r\n",
    "    label_mode='categorical',\r\n",
    "    seed=seed,\r\n",
    "    shuffle=True,\r\n",
    "    batch_size=batch_size,\r\n",
    "    validation_split=0.2,\r\n",
    "    subset='validation')\r\n",
    "\r\n",
    "test_ds = text_dataset_from_directory(\r\n",
    "    test_dir,\r\n",
    "    label_mode='categorical',\r\n",
    "    seed=seed,\r\n",
    "    shuffle=True,\r\n",
    "    validation_split=0.2,\r\n",
    "    subset='training',\r\n",
    "    batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "class_names = np.asarray(class_names)\n",
    "print(f\"nclasses: {len(class_names)}\")\n",
    "print(f'first 4 classes/users: {class_names[:4]}')\n",
    "for texts, labels in train_ds.take(1):\n",
    "    print(\"Shape of texts\", texts.shape)\n",
    "    print(f'Class of 2nd data point: {class_names[labels.numpy()[1].astype(bool)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 0\r\n",
    "for file in train_dir.glob('*/*.txt'):\r\n",
    "    with file.open() as f:\r\n",
    "        seq_len = 0\r\n",
    "        for line in f.readlines():\r\n",
    "            seq_len += len(line.split())\r\n",
    "#         print(seq_len)\r\n",
    "        if MAX_LEN < seq_len:\r\n",
    "            MAX_LEN = seq_len\r\n",
    "print(f\"length of largest article: {MAX_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, label in iter(val_ds):\n",
    "    index = np.argmax(label.numpy(), axis=1).astype(np.int)\n",
    "    print(f'Users of first batch: {class_names[index]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "Text vectorization includes the following tasks using `TextVectorization` layer:\n",
    "1. `Standardization`\n",
    "2. `Tokenization`\n",
    "3. `Vectorization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial run of the vectorization layers\n",
    "1. Make a text-only dataset (without labels), then call adapt\n",
    "2. Do not call adapt on test dataset to prevent data-leak\n",
    "3. train and save vocab to disk\n",
    "\n",
    "Note: Use it only for the first time or if vocab is not saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file vocab_C50.pkl already exists!\n",
      "object <class 'list'> saved to file vocab_C50.pkl!\n",
      "vocab size of vectorizer: 34000\n"
     ]
    }
   ],
   "source": [
    "from utils import save_object\n",
    "\n",
    "### Define vectorization layers\n",
    "VOCAB_SIZE = 34000\n",
    "MAX_LEN = 1400\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN\n",
    ")\n",
    "\n",
    "# Train the layers to learn a vocab\n",
    "train_text = train_ds.map(lambda text, lables: text)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "\n",
    "# Save the vocabulary to disk\n",
    "# Run this cell for the first time only\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "vocab_path = Path('vocab/vocab_C50.pkl')\n",
    "save_object(vocab, vocab_path)\n",
    "vocab_len = len(vocab)\n",
    "print(f\"vocab size of vectorizer: {vocab_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization layers from saved vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab\n",
    "from utils import load_object\n",
    "vocab_path = Path('vocab/vocab_C50.pkl')\n",
    "vocab = load_object(vocab_path)\n",
    "\n",
    "VOCAB_SIZE = 34000\n",
    "MAX_LEN  = 1400\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE, \n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN,\n",
    "    vocabulary=vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process dataset through layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "def prepare(ds):\n",
    "    return ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(vectorize)\n",
    "val_ds = val_ds.map(vectorize)\n",
    "test_ds = test_ds.map(vectorize)\n",
    "\n",
    "# Configure the datasets for fast training \n",
    "train_ds = prepare(train_ds)\n",
    "val_ds = prepare(val_ds)\n",
    "test_ds = prepare(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1400)\n"
     ]
    }
   ],
   "source": [
    "for text, label in train_ds.take(1):\n",
    "    print(text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling and Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded object from file vocab_C50.pkl\n",
      "(34000, 100)\n"
     ]
    }
   ],
   "source": [
    "##### Pretrained Glove Embeddings #####\r\n",
    "## Parse the weights\r\n",
    "from utils import load_object\r\n",
    "emb_dim = 100\r\n",
    "glove_file = Path(f'vocab/glove/glove.6B.{emb_dim}d.txt')\r\n",
    "emb_index = {}\r\n",
    "with glove_file.open(encoding='utf-8') as f:\r\n",
    "    for line in f.readlines():\r\n",
    "        values = line.split()\r\n",
    "        word = values[0]\r\n",
    "        coef = values[1:]\r\n",
    "        emb_index[word] = coef\r\n",
    "\r\n",
    "##### Getting embedding weights #####\r\n",
    "vocab = load_object(Path('vocab/vocab_C50.pkl'))\r\n",
    "emb_matrix = np.zeros((VOCAB_SIZE, emb_dim))\r\n",
    "for index, word in enumerate(vocab):\r\n",
    "    # get coef of word\r\n",
    "    emb_vector = emb_index.get(word)\r\n",
    "    if emb_vector is not None:\r\n",
    "        emb_matrix[index] = emb_vector\r\n",
    "print(emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1400, 100)         3400000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1390, 256)         281856    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 198, 256)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 198, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 198, 256)          394240    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 196, 128)          98432     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 63, 64)            24640     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6450      \n",
      "=================================================================\n",
      "Total params: 4,213,938\n",
      "Trainable params: 4,213,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\r\n",
    "lstm_model = models.Sequential([\r\n",
    "    layers.Embedding(VOCAB_SIZE, emb_dim, input_shape=(MAX_LEN,)),\r\n",
    "    layers.Conv1D(256, 11, activation='relu'),\r\n",
    "    layers.MaxPooling1D(7),\r\n",
    "    layers.Dropout(0.4),\r\n",
    "    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\r\n",
    "    layers.Conv1D(128, 3, activation='relu'),\r\n",
    "    layers.MaxPooling1D(3),\r\n",
    "    layers.Dropout(0.2),\r\n",
    "    layers.Conv1D(64, 3, activation='relu'),\r\n",
    "    layers.GlobalMaxPooling1D(),\r\n",
    "    layers.Dense(128, activation='relu'),\r\n",
    "    layers.Dense(50, activation='softmax')\r\n",
    "])\r\n",
    "\r\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1400, 100)         3400000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1390, 256)         281856    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 198, 256)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 198, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 198, 256)          394240    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 196, 128)          98432     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 63, 64)            24640     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6450      \n",
      "=================================================================\n",
      "Total params: 4,213,938\n",
      "Trainable params: 813,938\n",
      "Non-trainable params: 3,400,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.layers[0].set_weights([emb_matrix])\r\n",
    "lstm_model.layers[0].trainable = False\r\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\r\n",
    "\r\n",
    "optim = RMSprop(lr=1e-4)\r\n",
    "\r\n",
    "lstm_model.compile(\r\n",
    "    loss='CategoricalCrossentropy',\r\n",
    "    optimizer=optim,\r\n",
    "    metrics=['acc']\r\n",
    ")\r\n",
    "lstm_history = lstm_model.fit(\r\n",
    "    train_ds,\r\n",
    "    validation_data=val_ds,\r\n",
    "    epochs=100\r\n",
    ")\r\n",
    "\r\n",
    "lstm_model.save('models/base.h5')\r\n",
    "\r\n",
    "lstm_model.evaluate(test_ds)\r\n",
    "\r\n",
    "plot_history(lstm_history)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ml': conda)",
   "name": "python385jvsc74a57bd004a20cc0f25f2654a5fc5715c026c4c293afbc25926c593c301cab56769943bf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 5
}