{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 files belonging to 50 classes.\n",
      "Using 2000 files for training.\n",
      "Found 2500 files belonging to 50 classes.\n",
      "Using 500 files for validation.\n",
      "Found 2500 files belonging to 50 classes.\n"
     ]
    }
   ],
   "source": [
    "# Imports\r\n",
    "import keras\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "from pathlib import Path\r\n",
    "from utils import plot_history\r\n",
    "from keras import models, layers\r\n",
    "from keras.preprocessing import text_dataset_from_directory\r\n",
    "\r\n",
    "ds_dir = Path('data/C50/')\r\n",
    "train_dir = ds_dir / 'train'\r\n",
    "test_dir = ds_dir / 'test'\r\n",
    "seed = 1000\r\n",
    "batch_size = 16\r\n",
    "\r\n",
    "\r\n",
    "train_ds = text_dataset_from_directory(train_dir,\r\n",
    "                                     label_mode='int',\r\n",
    "                                     seed=seed,\r\n",
    "                                     shuffle=True,\r\n",
    "                                     validation_split=0.2,\r\n",
    "                                     subset='training')\r\n",
    "\r\n",
    "val_ds = text_dataset_from_directory(train_dir,\r\n",
    "                                      label_mode='int',\r\n",
    "                                      seed=seed,\r\n",
    "                                      shuffle=True,\r\n",
    "                                      validation_split=0.2,\r\n",
    "                                     subset='validation')\r\n",
    "\r\n",
    "test_ds = text_dataset_from_directory(test_dir,\r\n",
    "                                       label_mode='int',\r\n",
    "                                       seed=seed,\r\n",
    "                                       shuffle=True,\r\n",
    "                                       batch_size=batch_size)\r\n",
    "\r\n",
    "class_names = train_ds.class_names\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and Configure the datasets\r\n",
    "from utils import get_text, prepare_batched\r\n",
    "from transformers import DistilBertTokenizerFast\r\n",
    "\r\n",
    "AUTOTUNE = tf.data.AUTOTUNE\r\n",
    "\r\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\r\n",
    "\r\n",
    "train_ds = prepare_batched(train_ds, tokenizer)\r\n",
    "val_ds = prepare_batched(val_ds, tokenizer)\r\n",
    "test_ds = prepare_batched(test_ds, tokenizer)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the model\r\n",
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\r\n",
    "\r\n",
    "train_args = TFTrainingArguments(\r\n",
    "    output_dir='./results',\r\n",
    "    num_train_epochs=3,\r\n",
    "    per_device_eval_batch_size=64,\r\n",
    "    per_device_train_batch_size=16,\r\n",
    "    warmup_steps=500,\r\n",
    "    weight_decay=1e-2,\r\n",
    "    \r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ml': conda)",
   "name": "python385jvsc74a57bd004a20cc0f25f2654a5fc5715c026c4c293afbc25926c593c301cab56769943bf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}